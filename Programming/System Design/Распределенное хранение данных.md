---
создал заметку: 2024-08-01
tags:
  - system-design
---
### Описание

##### Репликация

**Бэкап** - это резервное копирование содержимого диска с целью последующего восстановления.

**Репликация** - это создание клона базы данных для быстрого подхвата функций поврежденной системы.

**1. Надежность:**
Поддержка резервной базы данных на случай потери основной

![](https://i.imgur.com/ezIvnGB.png)

**2. Масштабирование чтения:**
Снижение нагрузки на чтение за счет переноса части запросов на реплики

![](https://i.imgur.com/i2erjpr.png)
#### Виды ролей репликации:

**Master-Slave:** Данные записываются только в мастер-базу данных, а чтение может происходить с реплик (slave) или мастера. В случае сбоя мастера возможен простой на запись (downtime).

![](https://i.imgur.com/GZ2VvVQ.png)

При падении мастера нам некуда писать (Update, Insert, Delete). Решения:

**1. Failover:**

*Ручной выбор нового мастера:* админу пришло сообщение о том, что мастер упал, и он делает выбор нового мастера - руками
*Автоматический выбор:* новый мастер выбирается автоматически по какому-то принципу

Выбор нового мастера - это **медленный процесс**

![](https://i.imgur.com/HlWCXTY.png)

**2. Hot Standby**

Помечаем сразу же следующего мастера, то есть если упал мастер, то система уже будет знать кого брать на место следующего мастера

Реплика, которая выбрана в качестве резервного мастера должна работать в синхронном режиме. Это гарантирует, что все данные на мастер и slave синхронизированы и консистентны в любой момент времени.

*При данном подходе у нас помечена только одна реплика*

![](https://i.imgur.com/RfL8Bmz.png)

**Проблема Master-Slave: Split Brain**

Упал мастер, какая- то реплика стала новым мастером, прошлый мастер ожил и теперь в системе работают два мастера

Это состояние необходимо предотвращать с помощью специальных механизмов, например, система поколений:
- у каждого мастера - есть поколение
- текущий мастер поколения 5, пришел запрос поколения 4 - превращаем мастер поколения 4 в slave

![](https://i.imgur.com/G3FMWQK.png)

**Master-Master:**

- Появляются конфликты
- ﻿﻿Пишем в разные мастера
- ﻿﻿Читаем из слейвов или мастеров
- В случае падения мастера нет никакого downtime на запись

![](https://i.imgur.com/VKsUhq2.png)

*Конфликты:*

![](https://i.imgur.com/y1FbNyd.png)

Решения:
- **Last Write Wins (LWW)** — это стратегия разрешения конфликтов, при которой сохраняется последнее изменение данных на основе метки времени (timestamp) или некоторого другого порядка.

- **Ранг реплик** - основывается на установлении приоритета между узлами системы. При конфликте приоритет отдается изменению от узла с более высоким рангом.

- ﻿﻿**Решение конфликтов на клиенте** - решение конфликтов откладывается до клиента, который обрабатывает данные. Клиент принимает на себя ответственность за анализ и разрешение конфликтов.

- ﻿﻿**Conflict-Free Replicated Data Types (CRDT)** - это структуры данных, которые могут быть одновременно изменены на нескольких узлах без конфликта и без необходимости координации между узлами.

**Master-Less (используется редко):**

- Пишем в определенные ноды
- ﻿﻿Читаем с определенных нод

W - кол-во реплик в которые пишем
R - кол-во реплик из которых читаем

- W + R > N - гарантируется строгая  
    согласованность
- ﻿﻿W + R <= N - не гарантируется строгая  
    согласованность
- ﻿﻿R = 1 и W = N - система оптимизирована  
    для быстрого чтения
- ﻿﻿W = 1 и R = N - система оптимизирована  
    для быстрой записи

![](https://i.imgur.com/yOf0bZn.png)

#### Типы согласованности:

**Strong Consistency**

Обеспечивает, что все ноды имеют одинаковые данные после любой записи.

- работает медленно
- возможно большее количество ошибок: в один из узлов запись не удалась => вся операция не прошла

![](https://i.imgur.com/kzEx4PR.png)

Это достигается за счет *синхронной репликации*:
1. ﻿﻿﻿Запись транзакции в журнал (WAL)
2. ﻿﻿﻿Применение транзакции в движке
3. ﻿﻿﻿Отправка данных на все реплики
4. ﻿﻿﻿Получение подтверждения от всех реплик
5. ﻿﻿﻿Возвращение подтверждения клиенту

![](https://i.imgur.com/Kco5UmL.png)

**Eventual Consistency**

Гарантирует, что при отсутствии новых записей все копии данных станут одинаковыми *через некоторое время*.

Это достигается за счет *асинхронной репликации*:
1. ﻿﻿﻿Запись транзакции в журнал
2. ﻿﻿﻿Применение транзакции в движке
3. ﻿﻿﻿Возвращение подтверждения клиенту
4. Отправка данных на реплики

![](https://i.imgur.com/SjfqKZt.png)

Если пользователь отправил данные и пошел сразу же их читать, то этих данных еще может не быть на реплике:

![](https://i.imgur.com/6Gt60Qs.png)
#### Решения проблем с согласованностью

**Чтение собственных записей (Read Your Own Writes)** — это гарантия согласованности в распределенных системах, которая обеспечивает пользователю возможность видеть результаты своих собственных записей (изменений данных) после их выполнения. Это важно для приложений, где пользователи ожидают мгновенного отражения своих действий, таких как социальные сети или системы управления контентом.

Нужно отследить, когда пользователь в последний раз отправлял обновление. Если он отправил обновление в течение последней минуты, то запросы на чтение должен обрабатываться главным узлом

**Монотонное чтение (Monotonic Reads)** — это гарантия согласованности, которая обеспечивает, что если пользователь уже прочитал определенную версию данных, то все последующие чтения не покажут более старую версию этих данных.

Обеспечить, чтобы каждый пользователь всегда читал из одного и того же узла-последователя (разные пользователи могут читать с разных реплик)

**Согласованное префиксное чтение (Consistent Prefix Read)** — это гарантия согласованности, обеспечивающая, что при чтении данных из базы данных пользователи всегда видят операции в том порядке, в котором они были применены. Это означает, что если несколько операций были выполнены последовательно, пользователь никогда не увидит эффекты более поздней операции без видимости эффектов всех предшествующих операций.

**Полусинхронная репликация (Semisynchronous Replication)** — это подход к репликации данных, который стремится сочетать преимущества как синхронной, так и асинхронной репликации, обеспечивая баланс между производительностью и консистентностью.

1. ﻿﻿﻿Запись транзакции в журнал
2. ﻿﻿﻿Применение транзакции в движке
3. ﻿﻿﻿Отправка данных на реплики.
4. ﻿﻿﻿Получение подтверждения от реплики о получении изменений (применены они будут «когда-то потом»)
5. Возвращение подтверждения клиенту

![](https://i.imgur.com/Z1EpkTL.png)

**Lose-less Semisynchronous Replication** — это вариация полусинхронной репликации, предназначенная для дальнейшего снижения риска потери данных.

1. ﻿﻿﻿Запись транзакции в журнал
2. ﻿﻿﻿Отправка данных на реплики.
3. ﻿﻿﻿Получение подтверждения от реплики о получении изменений (применены они будут «когда-то потом»)
4. ﻿﻿﻿Применение транзакции в движке
5. ﻿﻿﻿Возвращение подтверждения

![](https://i.imgur.com/MghiLNL.png)
#### Форматы передачи данных

Источник передачи данных:
1. ﻿﻿﻿**push** - мастер рассылает данные репликам (PgSQL)
2. ﻿﻿﻿**pull** - реплики стягивают данные сами (MySQL)
##### 1. Statement-Based Replication (SBR)

В формате SBR передаются SQL-запросы (или утверждения), которые были выполнены на мастер-узле. Каждый запрос считается на каждой ноде (random()), unix_timestamp(), ...)
##### 2. Row-Based Replication (RBR)

- Передаются измененные строчки в бинарном виде (как правило, но можно и в строковом виде)
- Передаются сущности, даже если изменили одно поле (но можно настраивать full / minimal, ...)
##### 3. Mixed Replication

В формате Mixed Replication система может переключаться между SBR и RBR методами в зависимости от ситуации.
##### 4. Логическая репликация

Логическая репликация передает изменения данных на уровне логики приложения, например, в виде структурированных данных (кортежей), а не на уровне физических данных.
##### 5. Физическая репликация

Физическая репликация работает на уровне физических страниц данных и файлов базы данных, создавая точные копии данных на уровне байтов.

slave = master (байт в байт)
#### CAP теорема

CAP теорема — это фундаментальный принцип в области распределенных систем, который утверждает, что любая распределенная система может одновременно обеспечивать только два из следующих трех свойств:

**Согласованность (Consistency):**

Каждый запрос к системе должен получить одинаковый результат, даже если он направлен на разные узлы. Иными словами, после завершения операции записи все узлы системы видят одно и то же состояние данных.

![](https://i.imgur.com/YT5pLTm.png)

Пример: в банковской системе после перевода денег между счетами баланс должен быть одинаковым на всех серверах.

**Доступность (Availability):**

Каждый запрос, отправленный на живую ноду (узел) системы, должен завершиться успешным ответом в разумное время, даже если одна или несколько нод вышли из строя.

![](https://i.imgur.com/nDTVi7g.png)

Пример: веб-приложение должно отвечать на запросы пользователей, даже если один из серверов не отвечает.

**Устойчивость к разделению (Partition Tolerance):**

Система продолжает функционировать, несмотря на разрывы или разделения сети, которые могут нарушить связь между узлами.

Пример: в случае сетевого сбоя система должна продолжать обслуживать запросы и гарантировать, что данные не будут потеряны или повреждены.

**Компромиссы CAP:**

CAP теорема утверждает, что *невозможно достичь всех трех свойств одновременно*. Разработчики должны выбирать, какие **два из трех** свойств они хотят обеспечить, в зависимости от потребностей системы и бизнес-требований:

1. **CP (Consistency and Partition Tolerance):**
    
    - Системы обеспечивают согласованность и устойчивость к разделению, но могут жертвовать доступностью. Это означает, что в случае сетевого разделения некоторые части системы могут стать недоступными для обеспечения согласованности данных.
    - Пример: традиционные реляционные базы данных, такие как PostgreSQL в режиме репликации с сильной согласованностью.
    
2. **AP (Availability and Partition Tolerance):**
    
    - Системы обеспечивают доступность и устойчивость к разделению, но могут жертвовать строгой согласованностью. Это означает, что система будет продолжать работать, даже если некоторые узлы разделены, но данные могут быть не полностью синхронизированы.
    - Пример: многие NoSQL базы данных, такие как Cassandra или DynamoDB, где акцент делается на доступность и масштабируемость.
    
3. **CA (Consistency and Availability):**
    
    - Системы обеспечивают согласованность и доступность, но не могут гарантировать устойчивость к разделению. Это означает, что в случае сетевого сбоя система может потерять свою способность поддерживать согласованность или доступность.
    - Пример: распределенные системы с синхронной репликацией без учета устойчивости к сетевым сбоям.
#### Партиционирование

**Партиционирование** — это процесс разделения большой базы данных или таблицы на более мелкие, управляемые части, называемые *партициями*. Это помогает улучшить производительность, управляемость и масштабируемость системы. Партиционирование особенно полезно для обработки больших объемов данных, поскольку позволяет параллельно обрабатывать разные части данных, минимизируя время выполнения запросов и операций.

Желательно, чтобы это происходило прозрачным для приложения способом (**секции находятся на одном и том же инстансе базы данных**)

**Вертикальное партиционирование (Vertical Partitioning):**

- Данные разделяются по столбцам. Каждая партиция содержит только часть столбцов исходной таблицы.
- **Пример:** Таблица продуктов может быть разделена на две части: одна партиция содержит идентификаторы и названия продуктов, другая — цены и описания.

- **Преимущества:** Уменьшение объема данных, обрабатываемых при запросах, использующих только часть столбцов, что повышает производительность.
- **Недостатки:** Требует объединения данных из нескольких партиций при запросах, которые используют все столбцы.

![](https://i.imgur.com/8RzjrBF.png)

**Горизонтальное партиционирование (Horizontal Partitioning):**

- Данные разделяются по строкам. Каждая партиция содержит полный набор столбцов, но только часть строк.
- **Пример:** Таблица пользователей может быть разделена по диапазонам идентификаторов пользователей (например, от 1 до 1000 в одной партиции, от 1001 до 2000 в другой и так далее).

- **Преимущества:** Уменьшение размера таблиц, что ускоряет операции сканирования и упрощает управление данными.
- **Недостатки:** Может усложнить запросы, требующие данных из нескольких партиций.

![](https://i.imgur.com/sC5s7To.png)
#### Шардирование

**Шардирование** — это метод горизонтального разделения данных на несколько баз данных или инстансов, называемых "шардами". Каждый шард хранит подмножество данных и управляется отдельно. Этот подход помогает увеличить масштабируемость системы и улучшить производительность, распределяя нагрузку на несколько серверов.

![](https://i.imgur.com/gRn1Fdp.png)

![](https://i.imgur.com/sQyC9Uo.png)

![](https://i.imgur.com/jTt9EUs.png)

Шард реплицирует свои данные внутри себя (дополнительная отказоустойчивость)

![](https://i.imgur.com/QE9huxO.png)

##### Способы шардирования (распределение данных между шардами)

**Range-based Sharding (Диапазонное шардирование):**

- Данные распределяются по шардам на основе диапазонов значений ключа шардирования.
- **Пример:** Пользователи с ID от 1 до 1000 хранятся в одном шарде, от 1001 до 2000 — в другом и так далее.

![](https://i.imgur.com/zgvEL2E.png)

**Hash-based Sharding (Хеширование):**

- Данные распределяются по шардам на основе хеш-функции, примененной к ключу шардирования.
- **Пример:** Хеширование идентификатора пользователя для определения, в каком шарде будут храниться данные этого пользователя.

![](https://i.imgur.com/pb2UdWg.png)

**List-based Sharding (Список):**

- Данные распределяются по шардам на основе предопределенных списков значений.
- **Пример:** Данные разделяются по регионам (например, Европа, Азия, Америка) и каждая группа значений направляется в соответствующий шард.

**Directory-based Sharding (Каталожное шардирование):**

- Каталог или таблица маршрутизации используется для определения, в какой шард направить данные.
- **Пример:** Центральный каталог содержит информацию о том, какие данные хранятся в каждом шарде.

![](https://i.imgur.com/tRQmEy5.png)
#### Маршрутизация запросов (Routing)

Чтобы определить, в какой шард направить запрос, используется логика маршрутизации:

**Клиентский маршрутизатор (Client-side Routing):**

![](https://i.imgur.com/NXce40G.png)

- Клиентская часть приложения определяет, в какой шард направить запрос, используя алгоритм шардирования.
- **Преимущества:** Нет необходимости в дополнительной инфраструктуре для маршрутизации.
- **Недостатки:** Может быть сложно обновлять и поддерживать логику маршрутизации на клиенте.

**Прокси-сервер (Proxy-based Routing):**

![](https://i.imgur.com/aw3E46u.png)

- Прокси-сервер стоит между клиентом и шардами, обрабатывая запросы и маршрутизируя их в нужные шардов.
- **Преимущества:** Клиенты не знают о структуре шардирования, что упрощает архитектуру приложения.
- **Недостатки:** Дополнительный сетевой узел, который может стать узким местом или точкой отказа.

**Координатор (Coordinator-based Routing):**

![](https://i.imgur.com/OsiwrIh.png)

- Центральный сервер (координатор) управляет маршрутизацией запросов к шардам.
- **Преимущества:** Координатор может кэшировать данные и обеспечивать балансировку нагрузки.
- **Недостатки:** Увеличивает сложность инфраструктуры и может быть точкой отказа.
#### Перебалансировка и Решардинг

**Перебалансировка** — это процесс перераспределения данных между шардами для равномерного распределения нагрузки или при добавлении новых шардов. Перебалансировка может быть вызвана изменением объема данных или изменений в схеме шардирования.

1. ﻿﻿﻿Только чтение
2. ﻿﻿﻿Все данные неизменяемые (пишем в target, читаем из src и target)
3. ﻿﻿﻿Логическая репликация с src на target, после синхронизации переключаемся на target
4. ﻿﻿﻿Смешанный подход

**Использование виртуальных корзин (Virtual Buckets)**

Данные разделяются на мелкие "корзины", которые затем могут быть распределены по шардам. Это упрощает перебалансировку, так как перемещать нужно не данные, а корзины.

![](https://i.imgur.com/oJLoL8E.png)

**Решардинг (Resharding):** Это изменение схемы шардирования, которое может быть необходимо при добавлении новых серверов или изменении логики шардирования.

- нужно добавить / удалить ноды
- ﻿﻿исправление ошибок при выборе стратегии шардирования

**Проблема при решардинге:**

![](https://i.imgur.com/6u44wOs.png)

Добавился новый шард и те же самые ключи стали вести нас на другие шарды.

**Решение проблемы:**

**Consistent Hashing** (консистентное хеширование) — это метод распределения данных по множеству узлов (шардов) в распределенной системе, который минимизирует перераспределение данных при добавлении или удалении узлов. Это достигается за счет использования кольцевой структуры, где как данные, так и узлы проецируются в пространство значений хеш-функции.

![](https://i.imgur.com/f76HYJG.png)

На картинке выше есть проблемы:
- неравномерное распределение
- может произойти каскадный сбой (все шарды по цепочке упадут)

Решение этой проблемы - использование виртуальных шардов:

![](https://i.imgur.com/W7fCvEl.png)

**Rendezvous Hashing**, также известное как **Highest Random Weight (HRW) Hashing**, — это алгоритм распределения данных по множеству узлов. Этот метод позволяет определять узел для хранения данных так, чтобы минимизировать перераспределение данных при добавлении или удалении узлов.

![](https://i.imgur.com/gFHo5cv.png)

### Краткое содержание
